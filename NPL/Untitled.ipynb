{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"we don't need the given questions\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove whitespace from text\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    " \n",
    "input_str = \"   we don't need   the given questions\"\n",
    "remove_whitespace(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'sample', 'sentence', 'going', 'remove', 'stopwords', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# remove stopwords function\n",
    "def remove_stopwords(text):\n",
    "\tstop_words = set(stopwords.words(\"english\"))\n",
    "\tword_tokens = word_tokenize(text)\n",
    "\tfiltered_text = [word for word in word_tokens if word not in stop_words]\n",
    "\treturn filtered_text\n",
    "\n",
    "example_text = \"This is a sample sentence and we are going to remove the stopwords from this.\"\n",
    "remove_stopwords(example_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'scienc',\n",
       " 'use',\n",
       " 'scientif',\n",
       " 'method',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'mani',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# stem words in the list of tokenized words\n",
    "def stem_words(text):\n",
    "\tword_tokens = word_tokenize(text)\n",
    "\tstems = [stemmer.stem(word) for word in word_tokens]\n",
    "\treturn stems\n",
    "\n",
    "text = 'data science uses scientific methods algorithms and many types of processes'\n",
    "stem_words(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hp/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'science',\n",
       " 'use',\n",
       " 'scientific',\n",
       " 'methods',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'many',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# lemmatize string\n",
    "def lemmatize_word(text):\n",
    "\tword_tokens = word_tokenize(text)\n",
    "\t# provide context i.e. part-of-speech\n",
    "\tlemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "\treturn lemmas\n",
    "\n",
    "text = 'data science uses scientific methods algorithms and many types of processes'\n",
    "lemmatize_word(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PRP'),\n",
       " ('just', 'RB'),\n",
       " ('gave', 'VBD'),\n",
       " ('me', 'PRP'),\n",
       " ('a', 'DT'),\n",
       " ('scare', 'NN')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# convert text into word_tokens with their tags\n",
    "def pos_tagging(text):\n",
    "\tword_tokens = word_tokenize(text)\n",
    "\treturn pos_tag(word_tokens)\n",
    "\n",
    "pos_tagging('You just gave me a scare')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/hp/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    }
   ],
   "source": [
    "# download the tagset\n",
    "nltk.download('tagsets')\n",
    "\n",
    "# extract information about the tag\n",
    "nltk.help.upenn_tagset('NN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ bird/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ yellow/JJ bird/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# define chunking function with text and regular\n",
    "# expression representing grammar as parameter\n",
    "def chunking(text, grammar):\n",
    "\tword_tokens = word_tokenize(text)\n",
    "\n",
    "\t# label words with part of speech\n",
    "\tword_pos = pos_tag(word_tokens)\n",
    "\n",
    "\t# create a chunk parser using grammar\n",
    "\tchunkParser = nltk.RegexpParser(grammar)\n",
    "\n",
    "\t# test it on the list of word tokens with tagged pos\n",
    "\ttree = chunkParser.parse(word_pos)\n",
    "\t\n",
    "\tfor subtree in tree.subtrees():\n",
    "\t\tprint(subtree)\n",
    "\ttree.draw()\n",
    "\t\n",
    "sentence = 'the little yellow bird is flying in the sky'\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunking(sentence, grammar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Bill/NNP)\n",
      "  works/VBZ\n",
      "  for/IN\n",
      "  (ORGANIZATION GeeksforGeeks/NNP)\n",
      "  so/RB\n",
      "  he/PRP\n",
      "  went/VBD\n",
      "  to/TO\n",
      "  (GPE Delhi/NNP)\n",
      "  for/IN\n",
      "  a/DT\n",
      "  meetup/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "def named_entity_recognition(text):\n",
    "\t# tokenize the text\n",
    "\tword_tokens = word_tokenize(text)\n",
    "\n",
    "\t# part of speech tagging of words\n",
    "\tword_pos = pos_tag(word_tokens)\n",
    "\n",
    "\t# tree of word entities\n",
    "\tprint(ne_chunk(word_pos))\n",
    "\n",
    "text = 'Bill works for GeeksforGeeks so he went to Delhi for a meetup.'\n",
    "named_entity_recognition(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/hp/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
